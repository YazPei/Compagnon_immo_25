{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrichissement d’un CSV avec l’API DPE\n",
    "\n",
    "**Auteur :** Loick – Spécialiste GitHub & Jupyter  \n",
    "**Public visé :** Data scientists / développeurs (niveau intermédiaire)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction au sujet\n",
    "\n",
    "Ce notebook explique pas à pas comment enrichir un fichier CSV contenant des coordonnées GPS avec les données du jeu de données **DPE Logements existants** de l’ADEME via son API publique.  \n",
    "Nous verrons :\n",
    "- Pourquoi et quand enrichir vos données par API  \n",
    "- Les bonnes pratiques pour respecter les quotas  \n",
    "- Le code Python complet et commenté  \n",
    "- Comment analyser et valider les résultats obtenus  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Contexte théorique\n",
    "\n",
    "Avant de plonger dans le code, quelques points clefs :\n",
    "\n",
    "1. **Jeu de données DPE (`dpe03existant`)**  \n",
    "   - Hébergé sur data.ademe.fr (Data Fair).  \n",
    "   - Expose un endpoint `/lines` permettant de filtrer par géolocalisation (`geopoint`, `geo_distance`).  \n",
    "\n",
    "2. **Paramètres géographiques**  \n",
    "   - `geopoint` : latitude,longitude en WGS84 (EPSG:4326).  \n",
    "   - `geo_distance` : rayon de recherche autour de ce point (en mètres ou autre unité Elasticsearch).  \n",
    "\n",
    "3. **Quotas & limitations**  \n",
    "   - Anonyme : 600 requêtes / 60 s → ~10 req/s.  \n",
    "   - Authentifié : 1200 req / 60 s → ~20 req/s.  \n",
    "   - Ajoutez un `sleep` (≥0.1 s) pour rester sous les limites.\n",
    "\n",
    "4. **Workflow**  \n",
    "   - Lecture du CSV par morceaux (chunksize)  \n",
    "   - Cache local des appels pour éviter les doublons  \n",
    "   - Sauvegarde incrémentale du CSV enrichi  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Implémentation pas à pas\n",
    "\n",
    "### 3.1. Imports et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports & config\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Chemins\n",
    "INPUT_FILE  = \"merged_sales_data_dpe_enriched.csv\"\n",
    "OUTPUT_FILE = \"merged_sales_data_dpe_enriched_complet.csv\"\n",
    "\n",
    "# Traitement par chunks\n",
    "CHUNK_SIZE = 10_000\n",
    "\n",
    "# API parameters\n",
    "DISTANCE = 100         # rayon en mètre\n",
    "SLEEP_BETWEEN_CALLS = 0.11\n",
    "API_KEY = None         # ou votre clé ADEME\n",
    "\n",
    "# Mapping des champs API → colonnes CSV\n",
    "MAPPING_API_TO_DF = {\n",
    "    \"type_energie_principale_chauffage\": \"chauffage_energie\",\n",
    "    \"type_installation_chauffage_n1\":    \"chauffage_systeme\",\n",
    "    \"type_emetteur_installation_chauffage_n1\": \"chauffage_mode\",\n",
    "    \"annee_construction\":                \"annee_construction\",\n",
    "    \"etiquette_ges\":                     \"ges_class\"\n",
    "}\n",
    "COLS_TO_FILL = list(MAPPING_API_TO_DF.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Détection du séparateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Détection du séparateur CSV\n",
    "def detect_delimiter(fp):\n",
    "    with open(fp, 'r', encoding='utf-8') as f:\n",
    "        line = f.readline()\n",
    "    if ',' in line: return ','\n",
    "    if ';' in line: return ';'\n",
    "    if '\\t' in line: return '\\t'\n",
    "    return ','\n",
    "\n",
    "sep = detect_delimiter(INPUT_FILE)\n",
    "print(f\"Using separator: '{sep}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Fonction d’appel API avec cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Fonction d'appel API\n",
    "session = requests.Session()\n",
    "cache = {}\n",
    "\n",
    "def get_dpe_fields(lat, lon):\n",
    "    \"\"\"Retourne dict(fields) ou None.\"\"\"\n",
    "    try:\n",
    "        lat, lon = float(lat), float(lon)\n",
    "    except:\n",
    "        return None\n",
    "    key = (lat, lon)\n",
    "    if key in cache:\n",
    "        return cache[key]\n",
    "    geopoint = f\"{lat},{lon}\"\n",
    "    url = \"https://data.ademe.fr/data-fair/api/v1/datasets/dpe03existant/lines\"\n",
    "    params = {\"geopoint\": geopoint, \"geo_distance\": DISTANCE, \"size\": 1}\n",
    "    headers = {}\n",
    "    if API_KEY: headers[\"X-API-Key\"] = API_KEY\n",
    "\n",
    "    try:\n",
    "        r = session.get(url, params=params, headers=headers, timeout=5)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        fields = data.get(\"results\", [{}])[0].get(\"fields\", None)\n",
    "    except:\n",
    "        fields = None\n",
    "\n",
    "    cache[key] = fields\n",
    "    return fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Lecture, enrichissement et sauvegarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Enrichissement chunk par chunk\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    os.remove(OUTPUT_FILE)\n",
    "\n",
    "reader = pd.read_csv(INPUT_FILE, sep=sep, chunksize=CHUNK_SIZE)\n",
    "first = True\n",
    "\n",
    "for chunk_idx, df in enumerate(reader, start=1):\n",
    "    print(f\"\\n--- Chunk {chunk_idx} ---\")\n",
    "    # conversion coords\n",
    "    df[\"mapCoordonneesLatitude\"]  = pd.to_numeric(df[\"mapCoordonneesLatitude\"],  errors=\"coerce\")\n",
    "    df[\"mapCoordonneesLongitude\"] = pd.to_numeric(df[\"mapCoordonneesLongitude\"], errors=\"coerce\")\n",
    "\n",
    "    # mask : au moins une colonne cible manquante\n",
    "    mask = ~df[COLS_TO_FILL].notna().all(axis=1) & \\\n",
    "           df[\"mapCoordonneesLatitude\"].notna() & df[\"mapCoordonneesLongitude\"].notna()\n",
    "    to_fill = df[mask]\n",
    "\n",
    "    for idx in tqdm(to_fill.index, desc=\"Enrich\"):\n",
    "        lat = df.at[idx, \"mapCoordonneesLatitude\"]\n",
    "        lon = df.at[idx, \"mapCoordonneesLongitude\"]\n",
    "        fields = get_dpe_fields(lat, lon)\n",
    "        if fields:\n",
    "            for a, c in MAPPING_API_TO_DF.items():\n",
    "                if pd.isna(df.at[idx, c]) and a in fields:\n",
    "                    df.at[idx, c] = fields[a]\n",
    "        time.sleep(SLEEP_BETWEEN_CALLS)\n",
    "\n",
    "    # sauvegarde incrémentale\n",
    "    if first:\n",
    "        df.to_csv(OUTPUT_FILE, index=False, sep=sep)\n",
    "        first = False\n",
    "    else:\n",
    "        df.to_csv(OUTPUT_FILE, mode=\"a\", header=False, index=False, sep=sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyse des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Vérifications\n",
    "df_orig    = pd.read_csv(INPUT_FILE, sep=sep)\n",
    "df_enriched= pd.read_csv(OUTPUT_FILE, sep=sep)\n",
    "\n",
    "print(f\"Orig : {len(df_orig)} lignes\")\n",
    "print(f\"Enrichi : {len(df_enriched)} lignes\")\n",
    "\n",
    "for col in COLS_TO_FILL:\n",
    "    print(f\"{col} → {df_enriched[col].notna().sum():,} non-nuls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\t•\tWorkflow robuste : Le traitement par chunks, associé à un système de cache et à des pauses contrôlées, permet d’enrichir en toute fiabilité un gros fichier CSV via l’API DPE de l’ADEME, tout en respectant les quotas.\n",
    "\t•\tScalabilité :\n",
    "\t•\tLecture chunkée → évite de charger tout le fichier en mémoire.\n",
    "\t•\tCache des coordonnées → minimise les appels redondants.\n",
    "\t•\tModularité :\n",
    "\t•\tParamètres (taille de chunk, distance, pause API) facilement ajustables.\n",
    "\t•\tMapping API ↔ colonnes CSV simple à étendre à d’autres champs.\n",
    "\t•\tValidation facile :\n",
    "\t•\tScripts de vérification post‑traitement (comparaison du nombre de lignes, comptage des valeurs non-nulles) pour garantir que tous les enregistrements ont bien été parcourus et enrichis.\n",
    "\t•\tRessources complémentaires :\n",
    "\t•\tDocumentation API DPE : https://data.ademe.fr/data-fair/api/v1/datasets/dpe03existant\n",
    "\t•\tPandas : https://pandas.pydata.org/\n",
    "\t•\tMatplotlib : https://matplotlib.org/\n",
    "\t•\tSeaborn : https://seaborn.pydata.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
