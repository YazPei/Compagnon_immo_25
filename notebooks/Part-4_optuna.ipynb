{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fe3888b",
   "metadata": {},
   "source": [
    "# Pipeline de Feature Selection et d'Analyse Avancée\n",
    "Ce notebook intègre la sélection de caractéristiques, l'optimisation d'hyperparamètres, l'entraînement final, la visualisation des résultats de CV et l'analyse des résidus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bda730",
   "metadata": {},
   "source": [
    "## 1. Importation et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f06e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, pickle\n",
    "import pandas as pd, numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Constantes\n",
    "DATA_PATH = 'C:/Users/User/Downloads/drive-download-20250508T155351Z-1-001/'\n",
    "CV_SETUP_FILE = 'cv_setup.pkl'\n",
    "PARAM_FILE = '/mnt/data/param.py'\n",
    "XGB_FINAL_FILE = '/mnt/data/XGBoost_final.py'\n",
    "RESIDUS_FILE = '/mnt/data/Analyse_des_residus.py'\n",
    "CV_RESULTS_FILE = '/mnt/data/cv_results.pkl'\n",
    "N_SAMPLE = 150_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fb9f4d",
   "metadata": {},
   "source": [
    "## 2. Optimisation des hyperparamètres avec Optuna\n",
    "Chargement du code d'optimisation et exécution de l'étude Optuna pour XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5f1388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "# 1) Chargement des données\n",
    "DATA_PATH = 'C:/Users/User/Downloads/drive-download-20250508T155351Z-1-001/'\n",
    "X = pd.read_csv(os.path.join(DATA_PATH, 'X_train_encoded.csv'), sep=';')\n",
    "y = pd.read_csv(os.path.join(DATA_PATH, 'y_train.csv'), sep=';').values.ravel()\n",
    "\n",
    "# 2) Sous-échantillon pour accélérer l’optimisation\n",
    "X_sub, _, y_sub, _ = train_test_split(X, y, train_size=150_000, random_state=42)\n",
    "\n",
    "# 3) Objectif Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'device': 'cuda',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 12),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-3, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-3, 10.0, log=True),\n",
    "        'random_state': 42,\n",
    "        'verbosity': 0,\n",
    "    }\n",
    "    # split interne pour validation\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_sub, y_sub, test_size=0.2, random_state=42)\n",
    "    dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "    dval   = xgb.DMatrix(X_val, label=y_val)\n",
    "    bst = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=params['n_estimators'],\n",
    "        evals=[(dval, 'val')],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    y_pred = bst.predict(dval)\n",
    "    mse = mean_squared_error(y_val, y_pred)          # on récupère le MSE\n",
    "    rmse = np.sqrt(mse)                              # puis on en fait la racine\n",
    "    return rmse\n",
    "\n",
    "# 4) Lancement de l’étude\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=50, timeout=60*30)  # 50 essais max ou 30 min\n",
    "\n",
    "    print(\"→ Best RMSE:\", study.best_value)\n",
    "    print(\"→ Best params:\", study.best_params)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74df7cf1",
   "metadata": {},
   "source": [
    "## 3. Entraînement du modèle XGBoost final\n",
    "Utilisation des meilleurs hyperparamètres pour entraîner le modèle sur l'ensemble des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 1) Chargement complet\n",
    "DATA_PATH = 'C:/Users/User/Downloads/drive-download-20250508T155351Z-1-001/'\n",
    "X_train = pd.read_csv(os.path.join(DATA_PATH, 'X_train_encoded.csv'), sep=';')\n",
    "y_train = pd.read_csv(os.path.join(DATA_PATH, 'y_train.csv'), sep=';').values.ravel()\n",
    "X_test  = pd.read_csv(os.path.join(DATA_PATH, 'X_test_encoded.csv'),  sep=';')\n",
    "y_test  = pd.read_csv(os.path.join(DATA_PATH, 'y_test.csv'),  sep=';').values.ravel()\n",
    "\n",
    "# 2) Préparation DMatrix\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest  = xgb.DMatrix(X_test,  label=y_test)\n",
    "\n",
    "# 3) Paramètres optimaux trouvés\n",
    "best_params = {\n",
    "    'tree_method':      'gpu_hist',\n",
    "    'device':           'cuda',\n",
    "    'n_estimators':     1000,\n",
    "    'max_depth':        12,\n",
    "    'learning_rate':    0.024165459736487295,\n",
    "    'subsample':        0.7827805784475923,\n",
    "    'colsample_bytree': 0.9360622482907809,\n",
    "    'gamma':            1.9279857202175867,\n",
    "    'lambda':           5.317859770949592,\n",
    "    'alpha':            0.0016284032243054373,\n",
    "    'random_state':     42,\n",
    "    'verbosity':        1,\n",
    "}\n",
    "\n",
    "# 4) Entraînement final\n",
    "bst = xgb.train(\n",
    "    best_params,\n",
    "    dtrain,\n",
    "    num_boost_round=best_params['n_estimators'],\n",
    "    evals=[(dtrain, 'train')],\n",
    "    verbose_eval=50\n",
    ")\n",
    "\n",
    "# 5) Évaluation test\n",
    "y_pred = bst.predict(dtest)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2   = r2_score(y_test, y_pred)\n",
    "print(f\"→ Test RMSE: {rmse:.3f}, R2: {r2:.3f}\")\n",
    "\n",
    "# 6) Importances\n",
    "importances = bst.get_score(importance_type='gain')\n",
    "sorted_imp = sorted(importances.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"\\nTop 10 importances (gain) :\")\n",
    "for feat, imp in sorted_imp:\n",
    "    print(f\"  {feat:30s} → {imp:.4f}\")\n",
    "\n",
    "# 7) Sauvegarde\n",
    "bst.save_model(\"xgb_final.model\")\n",
    "print(\"\\nModèle sauvegardé sous `xgb_final.model`\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ec9be",
   "metadata": {},
   "source": [
    "## 4. Résultats de la validation croisée\n",
    "Chargement et affichage détaillé des performances de chaque pli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "cv_results = pickle.load(open(CV_RESULTS_FILE, 'rb'))\n",
    "df_cv = pd.DataFrame(cv_results)\n",
    "display(df_cv)\n",
    "plt.figure(figsize=(6,4))\n",
    "df_cv['RMSE'].hist()\n",
    "plt.title('Distribution de la RMSE par pli')\n",
    "plt.xlabel('RMSE')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6135a60",
   "metadata": {},
   "source": [
    "## 5. Analyse avancée des résidus\n",
    "Histogramme, Q–Q plot et tests de normalité pour évaluer la distribution des résidus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e6e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "\n",
    "# --- 1) Définissez exactement le dossier où sont vos CSV ---\n",
    "DATA_PATH = r\"C:/Users/User/Downloads/drive-download-20250508T155351Z-1-001\"\n",
    "\n",
    "# --- 2) Chargez vos données d'entraînement et de test ---\n",
    "X_train = pd.read_csv(os.path.join(DATA_PATH, 'X_train_encoded.csv'), sep=';')\n",
    "y_train = pd.read_csv(os.path.join(DATA_PATH, 'y_train.csv'),         sep=';').values.ravel()\n",
    "X_test  = pd.read_csv(os.path.join(DATA_PATH, 'X_test_encoded.csv'),  sep=';')\n",
    "y_test  = pd.read_csv(os.path.join(DATA_PATH, 'y_test.csv'),          sep=';').values.ravel()\n",
    "\n",
    "# --- 3) Rechargez le pipeline entraîné ---\n",
    "with open('best_pipe.pkl', 'rb') as f:\n",
    "    best_pipe = pickle.load(f)\n",
    "\n",
    "# --- 4) Prédictions & résidus ---\n",
    "y_pred_train = best_pipe.predict(X_train)\n",
    "res_train    = y_train - y_pred_train\n",
    "\n",
    "y_pred_test  = best_pipe.predict(X_test)\n",
    "res_test     = y_test  - y_pred_test\n",
    "\n",
    "# --- 5.1) Nuage de points résidus vs prédictions (train) ---\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(y_pred_train, res_train, alpha=0.2, s=10)\n",
    "plt.hlines(0, y_pred_train.min(), y_pred_train.max(), linestyles='--', linewidth=1)\n",
    "plt.xlabel(\"Prédictions (train)\")\n",
    "plt.ylabel(\"Résidus (train)\")\n",
    "plt.title(\"Résidus vs Prédictions — entraînement\")\n",
    "plt.show()\n",
    "\n",
    "# --- 5.2) Histogramme des résidus (test) ---\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(res_test, bins=50, density=True, alpha=0.7)\n",
    "plt.xlabel(\"Résidu\")\n",
    "plt.ylabel(\"Densité\")\n",
    "plt.title(\"Distribution des résidus — test\")\n",
    "plt.show()\n",
    "\n",
    "# --- 5.3) Q–Q plot des résidus (test) ---\n",
    "plt.figure(figsize=(6,4))\n",
    "st.probplot(res_test, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Q–Q plot des résidus — test\")\n",
    "plt.show()\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
