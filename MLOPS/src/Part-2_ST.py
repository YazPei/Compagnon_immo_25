#!/usr/bin/env python
# coding: utf-8

# # SÃ©rie Temporelle

# ## Preprocessing

# ### Import du dataset et split temporel

# #### Import du dataset avec comme index date

# In[6]:


import os
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

get_ipython().run_line_magic('matplotlib', 'inline')


# In[ ]:


from numba import njit, prange
import time

from concurrent.futures import ThreadPoolExecutor, as_completed
def travail_lourd(x):
    if x == 5:
        raise ValueError("Oups, erreur volontaire pour x=5")
    time.sleep(1)
    return x * x
inputs = list(range(12))
results = []
max_workers = min(4, os.cpu_count() or 1)
with ThreadPoolExecutor(max_workers=max_workers) as exe:
    futures = {exe.submit(travail_lourd, i): i for i in inputs}
    for fut in as_completed(futures):
        i = futures[fut]
        try:
            res = fut.result()
        except Exception as e:
            print(f"TÃ¢che {i} a levÃ© une exception : {e}")
            res = None
        results.append(res)
print("RÃ©sultats :", results)
@njit(parallel=True)
def somme_racines(n):
    tmp = np.zeros(n)
    for i in prange(n):
        tmp[i] = np.sqrt(i)
    return np.sum(tmp)


# In[8]:


import numpy as np
from numba import njit, prange

@njit(parallel=True)
def somme_racines(n):
    acc = 0.0
    for i in prange(n):
        acc += np.sqrt(i)
    return acc

# utilise tous les threads automatiquement
print(somme_racines(100_000_000))


# In[9]:


## paths
# folder_path_M = '/Users/maximehenon/Documents/GitHub/MAR25_BDS_Compagnon_Immo/'
folder_path_Y = "C:/Users/charl/OneDrive/Documents/Yasmine/DATASCIENTEST/FEV25-BDS-COMPAGNON"
# folder_path_C = '../data/processed/Sales'
# folder_path_L= '/Users/loick.d/Documents/Datascientest/Github immo/MAR25_BDS_Compagnon_Immo/'


# Load the dataset
# output_file = os.path.join(folder_path_M, 'df_sales_clean_ST.csv')
output_file = os.path.join(folder_path_Y, "df_sales_clean_ST.csv")
# output_file = os.path.join(folder_path_C, 'df_sales_clean_ST.csv')
# output_file = os.path.join(folder_path_L, 'df_sales_clean_ST.csv')

from tqdm import tqdm
import time

# Exemple d'une boucle avec une barre de progression
for i in tqdm(range(100)):
    time.sleep(0.1)  # Simuler un traitement long
    
chunksize = 100000  # Number of rows per chunk
chunks = pd.read_csv(
    output_file,
    sep=";",
    chunksize=chunksize,
    index_col="date",
    parse_dates=["date"],
    on_bad_lines="skip",
    low_memory=False,
)
# Process chunks
df_sales_clean_ST = pd.concat(chunk for chunk in chunks).sort_values(by="date")
print(df_sales_clean_ST.index.unique().sort_values().to_series().dt.day.unique())

print(df_sales_clean_ST.groupby("date")["INSEE_COM"].nunique().head(12))

display(df_sales_clean_ST.head())



# #### Ajout de la variable code postal

# In[ ]:


import geopandas as gpd
import pandas as pd
from shapely.geometry import Point

# === 2. CHARGEMENT DES POLYGONES DE CODES POSTAUX ===
## Paths
# folder_path_M = '/Users/maximehenon/Documents/GitHub/MAR25_BDS_Compagnon_Immo/'
folder_path_Y = "C:/Users/charl/OneDrive/Documents/Yasmine/DATASCIENTEST/FEV25-BDS-COMPAGNON/"
#folder_path_C = '../data/geo/json'


geo_file_name = 'contours-codes-postaux.geojson'
input_file = os.path.join(folder_path_Y, geo_file_name)

pcodes = gpd.read_file(input_file)[['codePostal', 'geometry']]
pcodes= pcodes.set_geometry('geometry')
pcodes = pcodes.to_crs(epsg=4326) 
print("Polygones chargÃ©s :", pcodes.shape)

# Creation de l'index spatial pour accÃ©lÃ©rer la recherche
_ = pcodes.sindex
df_sales_clean_ST = df_sales_clean_ST.reset_index()

# === 4. PRÃ‰TRAITEMENT GEO ===
df_base = df_sales_clean_ST.copy()

df_base = df_base.dropna(subset=['mapCoordonneesLatitude', 'mapCoordonneesLongitude'])
df_base['lat'] = df_base['mapCoordonneesLatitude']#.round(3)
df_base['lon'] = df_base['mapCoordonneesLongitude']#.round(3)
df_base['orig_index'] = df_base.index

# === 5. FONCTION DE TRAITEMENT SPATIAL D'UN CHUNK ===
def process_chunk(chunk, pcodes):
    chunk = chunk.copy()
    chunk['geometry'] = gpd.points_from_xy(chunk['lon'], chunk['lat'])
    gdf = gpd.GeoDataFrame(chunk, geometry='geometry', crs='EPSG:4326')
    gdf =  gdf.[gdf.is_valid]
    if gdf.crs != pcodes.crs:
        gdf = gdf.to_crs(pcodes.crs)
    _ = gdf.sindex 
    
    joined = gpd.sjoin(gdf, pcodes, how='left', predicate='within')
    return joined[['orig_index', 'codePostal']]  # retour minimal

# === 6. TRAITEMENT PAR CHUNKS POUR LIMITER LA MÃ‰MOIRE ===
chunksize = 100_000
results = []

for i in range(0, len(df_base), chunksize):
    #print(f"Traitement du chunk {i} â†’ {i+chunksize}")
    chunk = df_base.iloc[i:i+chunksize]
    result = process_chunk(chunk, pcodes)
    results.append(result)

# === 7. CONCATÃ‰NATION DES RÃ‰SULTATS ET MERGE FINAL ===
df_joined = pd.concat(results, ignore_index=True).drop_duplicates("orig_index")

df_sales_clean_ST['orig_index'] = df_sales_clean_ST.index  # pour merge

df_sales_clean_ST = df_sales_clean_ST.merge(df_joined[['orig_index', 'codePostal']], on="orig_index", how="left")
df_sales_clean_ST.drop(columns=['orig_index'], inplace=True)

# === 8. VÃ‰RIFICATION DU RÃ‰SULTAT ===
print(df_sales_clean_ST[['mapCoordonneesLatitude', 'mapCoordonneesLongitude', 'codePostal', 'date']].head())
print("Code postal manquant :", df_sales_clean_ST['codePostal'].isna().sum())


# #### AgrÃ©gation mensuelle 

# In[12]:


df_sales_clean_ST['date'] = pd.to_datetime(df_sales_clean_ST['date'], errors='coerce')
df_sales_clean_ST = df_sales_clean_ST.sort_values('date')

# DÃ©finir la colonne 'date' comme index
df_sales_clean_ST = df_sales_clean_ST.set_index('date')


# Creation des variable annÃ©e et mois et traiter le codePostal

df_sales_clean_ST["Year"] = df_sales_clean_ST.index.year
df_sales_clean_ST["Month"] = df_sales_clean_ST.index.month

df_sales_clean_ST["codePostal"] = df_sales_clean_ST["codePostal"].astype(str).str.replace(r"\.0$", "", regex=True)

datetime_cols = df_sales_clean_ST.select_dtypes(include=["datetime64[ns]"]).columns

for col in datetime_cols:
    print(f"Colonne datetime : {col}")
    print(df_sales_clean_ST[col].unique())


# corriger les valeurs de la colonne 'codePostal'
for code in df_sales_clean_ST["codePostal"].unique():
    if len(str(code)) < 5:
        code = str(code).zfill(5)
    # Convert 'codePostal' to string
df_sales_clean_ST["codePostal"] = df_sales_clean_ST["codePostal"].astype(str)
display(df_sales_clean_ST.head())


# #### Split Train et Test

# In[13]:


# SPLIT

train_clean = df_sales_clean_ST[
    (df_sales_clean_ST["Year"] < 2024) & (df_sales_clean_ST["Year"] > 2019)
]
test_clean = df_sales_clean_ST[df_sales_clean_ST["Year"] >= 2024]

display(test_clean.head())
test_clean = test_clean.reset_index()
train_clean = train_clean.reset_index()
display(train_clean["codePostal"].head())
display(train_clean[train_clean["codePostal"] =="75019"].head())


# #### Analyse des tendances

# ##### AgrÃ©gation des donnÃ©es par mois

# In[14]:


train_clean["departement"] = train_clean["codePostal"].astype(str).str[:2]
test_clean["departement"] = test_clean["codePostal"].astype(str).str[:2]

train_copy = train_clean.copy()

train_mensuel = (
    train_copy.groupby(["Year", "Month", "departement", "codePostal"])
    .agg(prix_m2_vente=("prix_m2_vente", "mean"))
    .reset_index()
)




# ##### Formattage des donnÃ©es temporelles

# In[ ]:


# Pour le mensuel
train_mensuel["date"] = pd.to_datetime(
    train_mensuel["Year"].astype(str) + "-" + train_mensuel["Month"].astype(str) + "-01"
)


# ##### Visualisation

# In[15]:


import plotly.express as px
import pandas as pd

Train_pour_graph = (
    train_clean.groupby(["Year", "Month"])
    .agg(prix_m2_vente=("prix_m2_vente", "mean"))
    .reset_index()
)

Train_pour_graph["date"] = pd.to_datetime(
    Train_pour_graph["Year"].astype(str)
    + "-"
    + Train_pour_graph["Month"].astype(str)
    + "-01"
)

###########################################
# Filtrage avec des dropdowns par departement

fig_mensuel_glob = px.line(
    Train_pour_graph,
    x="date",
    y="prix_m2_vente",
    title="Ã‰volution mensuelle du prix moyen au mÂ² ",
    labels={"date": "Date", "prix_m2_vente": "Prix moyen (â‚¬ / mÂ²)"},
)

fig_mensuel_glob.update_traces(mode="lines+markers")
fig_mensuel_glob.update_layout(
    title_x=0.5,
    title_y=0.95,
    title_font_size=20,
    xaxis_title="Date",
    yaxis_title="Prix moyen (â‚¬ / mÂ²)",
    hovermode="x unified",
)

fig_mensuel_glob.update_xaxes(title_font=dict(size=14), tickfont=dict(size=12))
fig_mensuel_glob.update_yaxes(title_font=dict(size=14), tickfont=dict(size=12))
fig_mensuel_glob.show()

###########################################
# Filtrage avec des dropdowns par departement
Train_pour_graph_cp = (
    train_clean.groupby(["Year", "Month", "departement"])
    .agg(prix_m2_vente=("prix_m2_vente", "mean"))
    .reset_index()
)
Train_pour_graph_cp["date"] = pd.to_datetime(
    Train_pour_graph_cp["Year"].astype(str)
    + "-"
    + Train_pour_graph_cp["Month"].astype(str)
    + "-01"
)

fig_mensuel = px.line(
    Train_pour_graph_cp,
    x="date",
    y="prix_m2_vente",
    color="departement",
    title="Ã‰volution mensuelle du prix moyen au mÂ² par departement",
    labels={
        "date": "Date",
        "prix_m2_vente": "Prix moyen (â‚¬ / mÂ²)",
        "departement": "departement",
    },
)

fig_mensuel.update_traces(mode="lines+markers")
fig_mensuel.update_layout(
    title_x=0.5,
    title_y=0.95,
    title_font_size=20,
    xaxis_title="Date",
    yaxis_title="Prix moyen (â‚¬ / mÂ²)",
    legend_title_text="departement",
    hovermode="x unified",
)

fig_mensuel.update_xaxes(title_font=dict(size=14), tickfont=dict(size=12))
fig_mensuel.update_yaxes(title_font=dict(size=14), tickfont=dict(size=12))

# Ajout de menus dÃ©roulants pour filtrer par departement et annÃ©e
departement = train_mensuel["departement"].unique()

# Menu pour filtrer par departement
departement_buttons = [
    dict(
        label=str(cp),
        method="update",
        args=[
            {"visible": [cp == c for c in departement]},
            {"title": f"Ã‰volution mensuelle pour le departement {cp}"},
        ],
    )
    for cp in departement
]

fig_mensuel.update_layout(
    updatemenus=[
        dict(
            active=0,
            buttons=departement_buttons,
            direction="down",
            showactive=True,
            x=1.15,
            xanchor="left",
            y=1.1,
            yanchor="top",
            pad={"r": 10, "t": 10},
            bgcolor="white",
            bordercolor="black",
            borderwidth=1,
        )
    ]
)

fig_mensuel.show()


# Pour mieux expliquer l'Ã©volution de la Target, nous ajoutons les taux immobilier Ã  notre set de donnÃ©e

# ## Enrichissement du dataset

# Pour mieux adresser le problÃ¨me, nous allons procÃ©der Ã  la segmentation des departements afin d'adresser les prix par segment gÃ©ographique
# Par exemple sur les biens immobiliers comme Paris, nous allons l'enrichir par des donnÃ©es de taux d'emprunt immobilier, taux de chomage, ...

# ### Extraction des indicateurs pour clustering

# #### Ajustement de la granularitÃ© pour le clustering
# 

# ##### Constat initial
# 
# Nous avons commencÃ© par rÃ©aliser un clustering par code postal, en utilisant des indicateurs agrÃ©gÃ©s par `codePostal` (prix moyen, Ã©cart-type, taux de croissance annuel moyen, etc.).  
# Cependant, au fil de l'analyse, nous avons constatÃ© que **de nombreux codes postaux disposaient de trÃ¨s peu de donnÃ©es**, parfois **moins de 5 ventes**.
# 
# Cela posait plusieurs problÃ¨mes :
# - Les statistiques calculÃ©es (moyenne, Ã©cart-type, TCAM) Ã©taient **peu fiables**.
# - Ces points faiblement renseignÃ©s pouvaient **brouiller le clustering** global.
# 
# ##### Seuil critique observÃ©
# 
# Nous avons observÃ© que :
# - Certains `codePostal` n'avaient **qu'une seule entrÃ©e**.
# - Le seuil de **10 observations** est un minimum gÃ©nÃ©ralement admis pour calculer des agrÃ©gations fiables.
# 
# ##### Solution mise en Å“uvre : **agrÃ©gation hybride**
# 
# Pour conserver Ã  la fois **la prÃ©cision locale** quand elle est disponible, et **la stabilitÃ© statistique** ailleurs, nous avons adoptÃ© une stratÃ©gie hybride :
# 
# - Si un `codePostal` contient **au moins 10 observations**, il est **conservÃ© tel quel**.
# - Sinon, il est **regroupÃ© au niveau du dÃ©partement** (`codePostal[:2]`).
# 
# Nous avons donc crÃ©Ã© une nouvelle colonne appelÃ©e `zone_mixte`, qui contient :
# - soit le code postal complet (`75001`, `13008`, etc.)
# - soit le code dÃ©partemental (`30`, `32`, `97`, etc.)
# 
# ##### Objectif
# 
# Cette approche permet de :
# - **PrÃ©server la finesse gÃ©ographique** dans les zones bien renseignÃ©es,
# - **Limiter le bruit** dans les zones sous-reprÃ©sentÃ©es,
# - AmÃ©liorer la **qualitÃ© du clustering** sans perdre d'information utile.
# 
# 

# #### CrÃ©ation de la variable hybride 'Zone Mixte' - Departement & Code Postal

# In[16]:


# On s'assure que les codes postaux sont bien au format 5 chiffres

train_clean["date"] = pd.to_datetime(train_clean["date"])
test_clean ["date"] = pd.to_datetime(test_clean ["date"])

# On garde les codes postaux frÃ©quents
cp_counts = train_clean["codePostal"].value_counts()
cp_frequents_str = set(cp_counts[cp_counts >= 10].index)


# Fonction hybride
def regroup_code(row, frequents_set):
    cp = row["codePostal"]
    if cp in frequents_set:
        return cp  # code postal dÃ©taillÃ©
    elif cp.startswith("97"):
        return cp[:3]  # DROM-COM
    elif cp.isdigit() and len(cp) == 5:
        return cp[:2]  # dÃ©partement
    else:
        return "inconnu"


#  Application sur train et test
train_clean["zone_mixte"] = train_clean.apply(
    lambda row: regroup_code(row, cp_frequents_str), axis=1
)

# Pour test_clean, on applique exactement la mÃªme logique sans recalculer les frÃ©quences
test_clean["zone_mixte"] = test_clean.apply(
    lambda row: regroup_code(row, cp_frequents_str), axis=1
)


# ##### construction d'un jeu d'entrainement avec la variable 'Zone Mixte' et un lag -1

# In[17]:


for df in (train_clean, test_clean):
    df.sort_values(["zone_mixte", "date"], inplace=True)
    df["prix_lag_1m"] = (
        df.groupby("zone_mixte")["prix_m2_vente"].shift(1)
    )
    df["prix_roll_3m"] = (
        df.groupby("zone_mixte")["prix_m2_vente"]
          .rolling(3, closed="left")
          .mean()
          .reset_index(level=0, drop=True)
    )

# contruire un jeu train et test avec les zones mixtes par mois
train_mensuel = (
    train_clean.groupby(["Year", "Month", "zone_mixte"])
    .agg(
        prix_m2_vente =("prix_m2_vente", "mean"),
        volume_ventes=("prix_m2_vente", "count"), 
    )
    .reset_index()
)


# ### CrÃ©ation de variable propre Ã  la segmentation gÃ©ographique
# Ces variables vont Ã©valuer la volatilitÃ© du prix, le taux de croissance, la moyenne des prix et la variabilitÃ©

# #### Calcul du taux de croissance annuel lissÃ©
# L'objectif est de prendre en compte la tendance globale de l'Ã©volution des prix par code postal,
# sur toute la pÃ©riode observÃ©e, en lissant les variations mois par mois.
# 

# In[18]:


import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# --- Chargement / check initial ---
# train_mensuel doit exister et contenir au minimum :
# ['zone_mixte', 'Year', 'Month', 'prix_m2_vente']
assert all(col in train_mensuel.columns 
           for col in ["zone_mixte", "Year", "Month", "prix_m2_vente"]),  "Il manque des colonnes dans train_mensuel."

# ---  Reconstruction du code postal propre ---
def get_code_postal_final(zone):
    s = str(zone)
    if s.isdigit() and len(s) == 5:
        return s
    if s.isdigit() and len(s) == 2:
        return s + "000"
    if s.startswith("97") and len(s) == 3:
        return s + "00"
    return "inconnu"

train_mensuel = train_mensuel.copy()
train_mensuel["codePostal_recons"] = (
    train_mensuel["zone_mixte"].apply(get_code_postal_final)
)

# 2. Date, ordinal et temps t (en mois depuis le dÃ©but)
train_mensuel["date"] = pd.to_datetime(
    train_mensuel["Year"].astype(str)
    + "-" +
    train_mensuel["Month"].astype(str).str.zfill(2)
    + "-01"
)
train_mensuel["ym_ordinal"] = train_mensuel["Year"] * 12 + train_mensuel["Month"]
train_mensuel = train_mensuel.sort_values(["codePostal_recons","date"])
train_mensuel["t"] = (
    train_mensuel
    .groupby("codePostal_recons")["ym_ordinal"]
    .transform(lambda x: x - x.min())
)

# 3. Lags et moyennes mobiles (rolling)
train_mensuel["prix_lag_1m"] = (
    train_mensuel
    .groupby("codePostal_recons")["prix_m2_vente"]
    .shift(1)
)
train_mensuel["prix_roll_3m"] = (
    train_mensuel
    .groupby("codePostal_recons")["prix_m2_vente"]
    .rolling(3, closed="left")
    .mean()
    .reset_index(level=0, drop=True)
)

# 4. Log-prix et TCAM
train_mensuel["log_prix"] = np.log(train_mensuel["prix_m2_vente"])

def compute_tcam(df):
    if len(df) < 2 or df["log_prix"].isna().any():
        return np.nan
    X = df[["t"]].values.reshape(-1,1)
    y = df["log_prix"].values
    coef = LinearRegression().fit(X, y).coef_[0]
    return (np.exp(coef) - 1) * 100 * 12

tcam_df = (
    train_mensuel
    .groupby("codePostal_recons")
    .apply(compute_tcam)
    .reset_index(name="tc_am_reg")
)

# 5. Assemblage final des features
train_mensuel = (
    train_mensuel
    .merge(tcam_df, on="codePostal_recons", how="left")
    .rename(columns={"prix_m2_vente": "prix_m2_mean"})
    # Ne drop que les lignes oÃ¹ tes features indispensables sont manquantes
    .dropna(subset=["prix_lag_1m", "prix_roll_3m", "tc_am_reg"])
    .reset_index(drop=True)
)

# On remet Ã  jour le log et t au cas oÃ¹ tu en auras besoin
train_mensuel["log_prix"]   = np.log(train_mensuel["prix_m2_mean"])
train_mensuel["t"]          = (
    train_mensuel
    .groupby("codePostal_recons")["ym_ordinal"]
    .transform(lambda x: x - x.min())
)

# VoilÃ  ton DataFrame propre, prÃªt pour clustering ou modÃ©lisation
print(train_mensuel.head())


# #### calcul des autres feature et integration du Taux de croissance annuel lissÃ©

# In[19]:


df_cluster_input = (
    train_mensuel
    .groupby("codePostal_recons")
    .agg(
        # on agrÃ¨ge les moyennes mensuelles calculÃ©es plus tÃ´t
        prix_m2_mean = ("prix_m2_mean", "mean"),
        prix_m2_std  = ("prix_m2_mean", "std"),
        prix_m2_max  = ("prix_m2_mean", "max"),
        prix_m2_min  = ("prix_m2_mean", "min"),
        # on agrÃ¨ge aussi tes lags & rolling
        avg_lag_1m   = ("prix_lag_1m",   "mean"),
        avg_roll_3m  = ("prix_roll_3m",  "mean"),
         )
    .assign(
        prix_m2_cv = lambda df: df["prix_m2_std"] / df["prix_m2_mean"]
    )
    .reset_index()
    # fusionne ensuite ton TCAM dÃ©jÃ  calculÃ©
    .merge(tcam_df, on="codePostal_recons", how="left")
)

print(df_cluster_input.head())


# ##### VÃ©rification de la qualitÃ© du taux de croissance annuel

# In[20]:


from sklearn.linear_model import LinearRegression

#  Visualisation de la tendance log-linÃ©aire
# On choisit un code postal pour visualiser la tendance
print(train_mensuel["codePostal_recons"].unique())
code_postal_exemple = "75019"  # Ã  adapter selon tes donnÃ©es

# Extraire les donnÃ©es correspondantes
df_exemple = train_mensuel[
    train_mensuel["codePostal_recons"] == code_postal_exemple
].dropna(subset=["log_prix", "t"])

print(df_exemple.shape)
print(df_exemple.head())
# Fit de la rÃ©gression
X = df_exemple[["t"]]
y = df_exemple["log_prix"]
model = LinearRegression().fit(X, y)
y_pred = model.predict(X)

# Plot
plt.figure(figsize=(10, 5))
plt.scatter(df_exemple["t"], y, label="log(prix rÃ©el)", color="blue")
plt.plot(df_exemple["t"], y_pred, label="rÃ©gression linÃ©aire", color="red", linewidth=2)
plt.title(f"Tendance log-linÃ©aire des prix pour le code postal {code_postal_exemple}")
plt.xlabel("Temps (annÃ©es depuis la premiÃ¨re observation)")
plt.ylabel("Log du prix au mÂ²")
plt.legend()
plt.grid(True)
plt.show()


# ## Clustering avec KMeans

# ### Recherche du nombre optimal de clusters

# In[21]:


from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split # pour un clustering applicable aux 2 modÃ¨les

# --- 0. Liste des features de clustering ---
features = [
    "prix_m2_std",
    "prix_m2_max",
    "prix_m2_min",
    "tc_am_reg",
    "prix_m2_cv",
    "avg_roll_3m",
    "avg_lag_1m",
]

# --- 1. PrÃ©parer X_train & conserver l'index ---
X = (
    df_cluster_input[features]
    .replace([np.inf, -np.inf], np.nan)
)
X_train = X.dropna()
X_train, X_test = train_test_split(X_train, test_size=0.2, random_state=42)
train_idx = X_train.index

# --- 2. Standardisation ---
scaler = StandardScaler().fit(X_train)
X_train_scaled = scaler.transform(X_train)

# --- 3. MÃ©thode du coude pour k de 2 Ã  9 ---
inertias = []
for k in range(2, 10):
    km = KMeans(n_clusters=k, random_state=42, n_init=10)
    inertias.append(km.fit(X_train_scaled).inertia_)

plt.figure()
plt.plot(range(2, 10), inertias, marker="o")
plt.title("Coude k-means â€“ Inertie intra-cluster")
plt.xlabel("Nombre de clusters (k)")
plt.ylabel("Inertie")
plt.grid(True)
plt.show()

# --- 4. Fit KMeans dÃ©finitif (ici k=4) ---
kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
labels = kmeans.fit_predict(X_train_scaled)

# On injecte ces labels DANS df_cluster_input
df_cluster_input.loc[train_idx, "cluster"] = labels.astype(int)



# ### CrÃ©ation du jeu de test avec les variables de train

# In[22]:


from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# â”€â”€ 1. Nettoyage des anciennes colonnes â”€â”€
to_drop = features + ["cluster", "cluster_label"]
test_clean = test_clean.drop(columns=to_drop, errors="ignore")

# â”€â”€ 2. RecrÃ©ation de zone_mixte et codePostal_recons â”€â”€
cp_counts    = train_clean["codePostal"].value_counts()
cp_frequents = set(cp_counts[cp_counts >= 10].index.astype(str))

test_clean["zone_mixte"] = test_clean.apply(
    lambda row: regroup_code(row, cp_frequents),
    axis=1
)
test_clean["codePostal_recons"] = test_clean["zone_mixte"].apply(get_code_postal_final)
test_clean.drop(columns=["zone_mixte"], inplace=True)

# â”€â”€ 3. Fusion des features agrÃ©gÃ©es â”€â”€
test_clean = test_clean.merge(
    df_cluster_input[["codePostal_recons"] + features],
    on="codePostal_recons",
    how="left"
)

# VÃ©rification que toutes les features sont prÃ©sentes
missing = set(features) - set(test_clean.columns)
if missing:
    raise ValueError(f"Il manque ces colonnes dans test_clean avant clustering : {missing}")

# â”€â”€ 4. Filtrage des lignes complÃ¨tes et prÃ©diction â”€â”€
# On ne clusterise que les lignes sans NaN
mask_valid = ~test_clean[features].isna().any(axis=1)
X_test_valid   = test_clean.loc[mask_valid, features]
X_test_scaled  = scaler.transform(X_test_valid)

test_clean.loc[mask_valid, "cluster"] = kmeans.predict(X_test_scaled)



# ### fixation des clusters

# In[23]:


# â”€â”€ 5. Mapping vers un label lisible â”€â”€
cluster_order = (
    df_cluster_input
    .groupby("cluster")["prix_m2_mean"]
    .mean()
    .sort_values()
    .index
    .tolist()
)
cluster_names = [
    "Zones rurales, petites villes stagnantes",
    "Centres urbains Ã©tablis, zones rÃ©sidentielles",
    "Banlieues, zones mixtes",
    "Zones tendues - secteurs spÃ©culatifs",
]
mapping = dict(zip(cluster_order, cluster_names))

df_cluster_input['cluster_label']=df_cluster_input['cluster'].map(mapping)
test_clean.loc[mask_valid, "cluster_label"] = test_clean.loc[mask_valid, "cluster"].map(mapping)

# â”€â”€ 6. RÃ©sultat â”€â”€
print(test_clean.loc[mask_valid, ["codePostal_recons"] + features + ["cluster", "cluster_label"]].head())
print(f"{mask_valid.sum()} lignes sur {len(test_clean)} assignÃ©es Ã  un cluster.")


# ### Visualisation

# In[24]:


cluster_palette = {
    "Zones rurales, petites villes stagnantes":    "#1f77b4",
    "Banlieues, zones mixtes":                    "#ff7f0e",
    "Centres urbains Ã©tablis, zones rÃ©sidentielles":"#2ca02c",
    "Zones tendues - secteurs spÃ©culatifs":        "#d62728",
}

# visualisation
sns.pairplot(
    df_cluster_input,
    vars=features,
    hue="cluster_label",
    hue_order=list(cluster_palette.keys()),
    palette=cluster_palette,
    corner=True            # pour nâ€™afficher que la moitiÃ© infÃ©rieure et gagner en lisibilitÃ©
)
plt.suptitle("Distribution des indicateurs par cluster (train)", y=1.02)
plt.show()



# | Cluster |  Couleur  | Niveau de prix |    VolatilitÃ©   |    Croissance (tc\_am\_reg)   | InterprÃ©tation Ã©conomique                                       |
# | :-----: | :-------: | :------------: | :-------------: | :---------------------------: | :-------------------------------------------------------------- |
# |    0    |  ðŸ”µ Bleu  |   **Faible**   | **TrÃ¨s faible** | **Faible / parfois nÃ©gative** | **Zones rurales / petites villes stagnantes**                   |
# |    1    | ðŸŸ  Orange |  **Moyen-bas** |   **ModÃ©rÃ©e**   |          **ModÃ©rÃ©e**          | **PÃ©riphÃ©ries et banlieues**                   |
# |    2    |  ðŸŸ¢ Vert  | **Moyen-haut** |   **ModÃ©rÃ©e**   |      **ModÃ©rÃ©e Ã  bonne**      | **Centres urbains Ã©tablis, marchÃ©s rÃ©sidentiels stables**       |
# |    3    |  ðŸ”´ Rouge | **TrÃ¨s Ã©levÃ©** |    **Ã‰levÃ©e**   |           **Forte**           | **Zones tendues / spÃ©culatives (luxe, hypercentre, littoralâ€¦)** |
# 

# ### Visualisation sur une map

# In[25]:


import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from shapely.geometry import Point
import matplotlib.patches as mpatches

# â”€â”€ 0. PrÃ©parer la liste des codes postaux frÃ©quents â”€â”€
cp_counts       = train_clean["codePostal"].value_counts()
cp_frequents_str = set(cp_counts[cp_counts >= 10].index.astype(str))

# â”€â”€ 1. Fonction â€œstring-onlyâ€ pour regrouper les codes postaux â”€â”€
def regroup_code_str(cp: str, freq_set: set) -> str:
    if cp in freq_set:
        return cp
    if cp.startswith("97"):
        return cp[:3]
    if cp.isdigit() and len(cp) == 5:
        return cp[:2]
    return "inconnu"

# â”€â”€ 2. Calculer les centroÃ¯des (lat/lon moyennes) par codePostal â”€â”€
coord_cp = (
    train_clean
    .dropna(subset=["mapCoordonneesLatitude", "mapCoordonneesLongitude"])
    .groupby("codePostal")[["mapCoordonneesLatitude","mapCoordonneesLongitude"]]
    .mean()
    .reset_index()
)

# â”€â”€ 3. Appliquer le regroupement et reconstruire codePostal_recons â”€â”€
coord_cp["zone_mixte"]        = coord_cp["codePostal"].astype(str).apply(
    lambda cp: regroup_code_str(cp, cp_frequents_str)
)
coord_cp["codePostal_recons"] = coord_cp["zone_mixte"].apply(get_code_postal_final)

# â”€â”€ 4. Fusionner avec votre df_cluster_input (qui porte cluster & cluster_label) â”€â”€
geo_df = pd.merge(
    df_cluster_input.reset_index(),  # attention: index doit devenir col. rÃ©indexez sinon
    coord_cp[["codePostal_recons","mapCoordonneesLatitude","mapCoordonneesLongitude"]],
    on="codePostal_recons",
    how="left"
).dropna(subset=["mapCoordonneesLatitude","mapCoordonneesLongitude"])

# â”€â”€ 5. Transformer en GeoDataFrame â”€â”€
geometry = [
    Point(xy) for xy in zip(
        geo_df["mapCoordonneesLongitude"],
        geo_df["mapCoordonneesLatitude"]
    )
]
geo_df = gpd.GeoDataFrame(geo_df, geometry=geometry, crs="EPSG:4326")

# Optionnel : ne garder que la mÃ©tropole
geo_df = geo_df[~geo_df["codePostal_recons"].str.startswith(("97","98"))]

# â”€â”€ 6. Choisir une palette de couleurs sur les labels â”€â”€


# â”€â”€ 7. Tracer la carte en boucle pour une lÃ©gende propre â”€â”€
fig, ax = plt.subplots(figsize=(10,12))
for lbl, color in cluster_palette.items():
    subset = geo_df[geo_df["cluster_label"] == lbl]
    subset.plot(
        ax=ax,
        color=color,
        markersize=25,
        alpha=0.7,
        label=lbl
    )
ax.legend(title="Type de zone", loc="lower left", fontsize=10, title_fontsize=12)
ax.set_title("Clusters immobiliers en France mÃ©tropolitaine", fontsize=14)
ax.axis("off")
plt.show()

# â”€â”€ 8. Boxplots explicatifs par cluster â”€â”€
features_box = ["prix_m2_mean","prix_m2_std","tc_am_reg","prix_m2_cv","avg_roll_3m","avg_lag_1m"]
sns.set_theme(style="whitegrid", palette="pastel")

order = list(cluster_palette.keys())

for feat in features_box:
    plt.figure(figsize=(10, 6))
    
    ax = sns.boxplot(
        y="cluster_label",           # on bascule en horizontal
        x=feat,
        data=geo_df,
        order=order,
        palette=cluster_palette,
        notch=True,                  # crans
        showfliers=False,            # pas les outliers extrÃªmes
        width=0.6
    )
    
    # Superposer les moyennes
    means = geo_df.groupby("cluster_label")[feat].mean().reindex(order)
    for i, m in enumerate(means):
        ax.plot(m, i, marker="D", color="black", label="_nolegend_")
    
    ax.set_title(f"{feat} par cluster", fontsize=14)
    ax.set_xlabel(feat.replace("_", " "), fontsize=12)
    ax.set_ylabel("")  # on conserve seulement le label des clusters
    plt.tight_layout()
    plt.show()


# In[26]:


# Export des rÃ©sultats pour l'integration dans le modÃ¨le LGBM
# 1. SÃ©lectionner et dÃ©dupliquer le mapping
clusters_st = (
    df_cluster_input[["codePostal_recons", "cluster"]]
    .drop_duplicates(subset="codePostal_recons")
)
folder_path_Y = "C:/Users/charl/OneDrive/Documents/Yasmine/DATASCIENTEST/FEV25-BDS-COMPAGNON"
# 2. Sauvegarder dans un CSV pour rÃ©utilisation ultÃ©rieure
clusters_st.to_csv(os.path.join(folder_path_Y, "clusters_st.csv"), sep=";", index=True, encoding="utf-8")


print("Export clusters_st.csv gÃ©nÃ©rÃ© avec",
      len(clusters_st), "entrÃ©es (zones).")


# Nous sommes bien sur les clusters suivants:
# 
# - **Cluster 3 (rouge) â€” zone de luxe / tendue** 
# Clairement sÃ©parÃ© en haut Ã  droite de presque tous les nuages de points.
# Prix trÃ¨s Ã©levÃ©s (mean, max, min), dispersion (std) forte.
# TCAM (tc_am_reg) souvent positif.
# TrÃ¨s cohÃ©rent avec des zones chÃ¨res, touristiques ou spÃ©culatives.
# 
# 
# - **Cluster 0 (bleu) â€”  ville dense, mature** zones Ã  prix modÃ©rÃ©ment Ã©levÃ©s mais stables
# Prix moyens comparables au cluster orange, voire lÃ©gÃ¨rement supÃ©rieurs.
# Variance (Ã©cart-type) plus faible : le marchÃ© est plus homogÃ¨ne.
# TCAM souvent modÃ©rÃ© â†’ zones matures et stabilisÃ©es, comme des centres-villes Ã©tablis ou zones rÃ©sidentielles stables.
# 
# - **Cluster 1 (orange) â€” zones pÃ©riurbaines ou dynamiques secondaires**:  ville mixte, pÃ©riurbaine ou segmentÃ©e
# Prix moyens similaires au cluster bleu.
# Variance plus forte â†’ marchÃ© plus dispersÃ©, peut-Ãªtre entre anciens et nouveaux quartiers, zones pÃ©riurbaines en mutation.
# TCAM modÃ©rÃ©ment positif â†’ marchÃ©s dynamiques ou en rattrapage, mais moins structurÃ©s que le bleu.
# 
# 
# - **Cluster 2 (vert) â€” zones rurales / peu dynamiques**
# Prix trÃ¨s bas et trÃ¨s homogÃ¨nes.
# TrÃ¨s peu de variance â†’ marchÃ© stagnant.
# TCAM parfois nÃ©gatif â†’ zones en dÃ©croissance ou stagnation.
# 
# 

# ## Varifications et export des donnÃ©es

# In[27]:


# Etape 4: export des donnÃ©es
display(test_clean.head())
display(test_clean["cluster"].value_counts())
# Et en pourcentage du total
print(test_clean["cluster"].value_counts(normalize=True) * 100)


# ## PrÃ©paration Encodage des facteurs exogÃ¨nes pour SARIMAX

# ### CrÃ©ation d'une liste de facteurs exogÃ¨nes

# In[28]:


variables_exp = ["taux_rendement_n7", "loyer_m2_median_n7","y_geo", "x_geo", "z_geo", "dpeL", "nb_pieces", 'IPS_primaire','rental_yield_pct']


# Pour notre modÃ©lisation, nous allons choisir SARIMAX
# Pour cela nous aurons besoin de preprocesser et encoder nos facteurs exogÃ¨nes

# ### Encodage des facteurs exogÃ¨nes
# Nous allons prendre les Top 10 features issus de la feature selection (Part-2)
# 'taux_rendement_n7', 'loyer_m2_median_n7', 'y_geo', 'x_geo', 'z_geo', 'taux_rendement_n6', 'nb_pieces'
# 
# Nous allons Ã©galement ajouter le taux d'emprunt 20 ans

# #### Facteurs exogÃ¨nes : encodage et standardisation

# ##### Encodage de la variable gÃ©ographique

# In[29]:


# ENCODAGE DES VARIABLES GEOGRAPHIQUES
import numpy as np

lat_rad = np.radians(train_clean["mapCoordonneesLatitude"].values)
lon_rad = np.radians(train_clean["mapCoordonneesLongitude"].values)

# Projection sur la sphÃ¨re unitÃ© :

### X_Train ###
train_clean["x_geo"] = np.cos(lat_rad) * np.cos(lon_rad)
train_clean["y_geo"] = np.cos(lat_rad) * np.sin(lon_rad)
train_clean["z_geo"] = np.sin(lat_rad)

### X_Test ###
lat_rad_test = np.radians(test_clean["mapCoordonneesLatitude"].values)
lon_rad_test = np.radians(test_clean["mapCoordonneesLongitude"].values)
test_clean["x_geo"] = np.cos(lat_rad_test) * np.cos(lon_rad_test)
test_clean["y_geo"] = np.cos(lat_rad_test) * np.sin(lon_rad_test)
test_clean["z_geo"] = np.sin(lat_rad_test)

# Les valeurs retournÃ©s sont comprises entre -1 et 1
# z est la latitude absolue (Nord /sud)
# x > 0 â†’ vers lâ€™Est (Greenwich â†’ 90Â° E)
# x < 0 â†’ vers lâ€™Ouest (Greenwich â†’ 90Â° O)
# y > 0 â†’ moitiÃ© Nord de lâ€™Ã©quateur (longitudes entre 0Â° et 180Â° E)
# y < 0 â†’ moitiÃ© Sud (longitudes entre 0Â° et 180Â° O)

# suppression des colonnes Latitude et Longitude
train_clean = train_clean.drop(
    columns=["mapCoordonneesLongitude", "mapCoordonneesLatitude"]
)
test_clean = test_clean.drop(
    columns=["mapCoordonneesLongitude", "mapCoordonneesLatitude"]
)

# Verification
print(test_clean.head())
print(train_clean.head())


# ##### Encodage dpeL

# In[30]:


from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline


# creation d'une pipeline pour faire un imputer et un encodage
impute = SimpleImputer(strategy="most_frequent")
encode = OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1)
train_clean["dpeL"] = train_clean["dpeL"].astype(str)
test_clean["dpeL"] = test_clean["dpeL"].astype(str)
# On crÃ©e une pipeline pour le prÃ©traitement
pipeline = Pipeline(steps=[("imputer", impute), ("encoder", encode)])
# On applique la pipeline sur les colonnes catÃ©gorielles

train_clean["dpeL"] = pipeline.fit_transform(train_clean["dpeL"].values.reshape(-1, 1))
test_clean["dpeL"] = pipeline.transform(test_clean["dpeL"].values.reshape(-1, 1))

# Afficher les rÃ©sultats
print("train_clean['dpeL'] aprÃ¨s transformation :")
print(train_clean["dpeL"].unique())
print("test_clean['dpeL'] aprÃ¨s transformation :")
print(test_clean["dpeL"].unique())


# ### Ajout de la variable cluster Ã  train_data (donnÃ©e non agrÃ©gÃ©e)

# In[31]:


# 2. Positionnez df_cluster_input pour un mapping rapide
cluster_map = df_cluster_input.set_index("codePostal_recons")
train_clean["codePostal_recons"] = (train_clean["zone_mixte"].apply(get_code_postal_final))

train_clean['cluster'] = train_clean["codePostal_recons"].map(cluster_map['cluster'])

# 4. Vos autres features propres Ã  train_clean restent :
#    "taux_rendement_n7", "loyer_m2_median_n7", "y_geo", "x_geo", "z_geo"

# RÃ©sultat : train_clean contient maintenant toutes les variables_exp
print(train_clean.head())



# 

# ##### Standardisation des facteurs exogÃ¨nes

# In[32]:


# Standardisation des variables numÃ©riques
from sklearn.preprocessing import StandardScaler


# CrÃ©er une instance de StandardScaler
scaler = StandardScaler()

# Ajuster le scaler sur les donnÃ©es d'entraÃ®nement
train_clean[variables_exp] = scaler.fit_transform(train_clean[variables_exp])

# Appliquer la transformation sur les donnÃ©es de test
test_clean[variables_exp] = scaler.transform(test_clean[variables_exp])

# VÃ©rification de la standardisation
print(train_clean[variables_exp].head())
print(test_clean[variables_exp].head())


# ### Creation d'un dataframe Monthly avec variables standardisÃ©s pour SARIMAX

# In[33]:


# Regroupement mensuel par cluster â€“ uniquement sur le train
variables_exp = [
    col for col in variables_exp if col not in ("cluster", "date")
]  # Regroupement mensuel par cluster (train uniquement)
# On regroupe par cluster et date
agg_cluster_monthly = (
    train_clean.groupby(["cluster", "date"], as_index=False)
    .agg({**{"prix_m2_vente": "mean"}, **{col: "mean" for col in variables_exp  }})
    .reset_index()
)

# Ajouter un indicateur split train/test pour plus tard (test sera prÃ©dit sÃ©parÃ©ment)
agg_cluster_monthly["split"] = "train"

# Export sÃ©curisÃ© sans data leak
# agg_cluster_monthly.to_csv("agg_cluster_monthly.csv", index=False)



# Regroupement mensuel par cluster (test uniquement)
agg_cluster_monthly_test = test_clean.groupby(["cluster", "date"], as_index=False).agg(
    {"prix_m2_vente": "mean", **{col: "mean" for col in variables_exp}}
)
# Ajouter un indicateur split
agg_cluster_monthly_test["split"] = "test"

# Export pour inspection
# agg_cluster_monthly_test.to_csv("agg_cluster_monthly_test.csv", index=False)


# #### AgrÃ©gation par mois et CrÃ©ation de la variable taux d'emprunt immobilier

# In[ ]:


# !pip install openpyxl


# In[34]:


train_periodique_q12 = (
    agg_cluster_monthly[agg_cluster_monthly["split"] == "train"]
    .set_index("date")
    .drop(columns=["split"])
)
test_periodique_q12 = (
    agg_cluster_monthly_test[agg_cluster_monthly_test["split"] == "test"]
    .set_index("date")
    .drop(columns=["split"])
)


# display(train_periodique_q12.head(5))

##############################################################################
# Importer les donnÃ©es de taux d'intÃ©rÃªt
################################################################################
# Chemins d'accÃ¨s aux fichiers

# chemin_taux_M = '/Users/maximehenon/Documents/GitHub/MAR25_BDS_Compagnon_Immo/data'
chemin_taux_Y = ("C:/Users/charl/OneDrive/Documents/Yasmine/DATASCIENTEST/FEV25-BDS-COMPAGNON/data")
# chemin_taux_C = '../data/banking'
# chemin_taux_L = '/Users/loick.d/Documents/Datascientest/Github immo/MAR25_BDS_Compagnon_Immo/data'

chemin_taux = os.path.join(chemin_taux_Y, "Taux immo.xlsx")
# chemin_taux = os.path.join(chemin_taux_C, 'Taux immo.xlsx')
# chemin_taux = os.path.join(chemin_taux_L, 'Taux immo.xlsx')
# chemin_taux = os.path.join(chemin_taux_M, 'Taux immo.xlsx')

# Importer les taux d'intÃ©rÃªt
import pandas as pd

taux = pd.read_excel(chemin_taux)
taux["date"] = pd.to_datetime(taux["date"], format="%Y-%m-%d")
taux = taux.set_index("date")
taux["taux"] = (
    taux["10 ans"].str.replace("%", "").str.replace(",", ".").str.strip().astype(float)
)
# display(taux.head(5))

# Fusionner les donnÃ©es de taux d'intÃ©rÃªt avec les donnÃ©es d'agrÃ©gation mensuelle
train_periodique_q12 = train_periodique_q12.merge(
    taux, left_index=True, right_index=True, how="left"
)
test_periodique_q12 = test_periodique_q12.merge(
    taux, left_index=True, right_index=True, how="left"
)

# VÃ©rification de la fusion
# display(train_periodique_q12.head(5))


# Standardisation des taux d'intÃ©rÃªt
scal = StandardScaler()
train_periodique_q12["taux"] = scal.fit_transform(train_periodique_q12[["taux"]])
test_periodique_q12["taux"] = scal.transform(test_periodique_q12[["taux"]])
# VÃ©rification de la standardisation
# print(train_periodique_q12.head())
# print(test_periodique_q12['taux'].head())
train_periodique_q12 = train_periodique_q12.reset_index()
test_periodique_q12 = test_periodique_q12.reset_index()

train_periodique_q12["prix_m2_vente"] = np.log(train_periodique_q12["prix_m2_vente"])
test_periodique_q12["prix_m2_vente"] = np.log(test_periodique_q12["prix_m2_vente"])


# Ne garder que les colonnes variables_exp
variables_exp = ["taux_rendement_n7", "loyer_m2_median_n7","y_geo", "x_geo", "z_geo", "dpeL", "nb_pieces", 'IPS_primaire','rental_yield_pct', 'taux']

train_periodique_q12 = train_periodique_q12[
    variables_exp + ["prix_m2_vente", "cluster", "date"]
]
test_periodique_q12 = test_periodique_q12[
    variables_exp + ["prix_m2_vente", "cluster", "date"]
]
# VÃ©rification de la structure finale
print(train_periodique_q12.head())
print(test_periodique_q12.head())


# ## Export des datasets

# In[35]:


# # Enregistrer le DataFrame final
train_periodique_q12.to_csv(
    os.path.join(folder_path_Y, "train_periodique_q12.csv"), sep=";", index=True
)
test_periodique_q12.to_csv(
    os.path.join(folder_path_Y, "test_periodique_q12.csv"), sep=";", index=True
)

# folder_path_M = ''
# folder_path_L = ''
# folder_path_C = '../data/processed/Sales'

# train_periodique_q12.to_csv(os.path.join(folder_path_C, 'train_periodique_q12.csv'), sep=';', index=True)
# test_periodique_q12.to_csv(os.path.join(folder_path_C, 'test_periodique_q12.csv'), sep=';', index=True)

# train_periodique_q12.to_csv(os.path.join(folder_path_M, 'train_periodique_q12.csv'), sep=';', index=True)
# test_periodique_q12.to_csv(os.path.join(folder_path_M, 'test_periodique_q12.csv'), sep=';', index=True)

# train_periodique_q12.to_csv(os.path.join(folder_path_L, 'train_periodique_q12.csv'), sep=';', index=True)
# test_periodique_q12.to_csv(os.path.join(folder_path_L, 'test_periodique_q12.csv'), sep=';', index=True)


# Enregistrer les dataset Train_clean et test_clean
train_clean.to_csv(os.path.join(folder_path_Y, "train_clean_ST.csv"), sep=";", index=True)
test_clean.to_csv(os.path.join(folder_path_Y, "test_clean_ST.csv"), sep=";", index=True)

# train_clean.to_csv(os.path.join(folder_path_C, "train_clean_ST.csv"), sep=";", index=True)
# test_clean.to_csv(os.path.join(folder_path_C, "test_clean_ST.csv"), sep=";", index=True)

# train_clean.to_csv(os.path.join(folder_path_M, 'train_clean.csv'), sep=';', index=True)
# test_clean.to_csv(os.path.join(folder_path_M, 'test_clean.csv'), sep=';', index=True)

# train_clean.to_csv(os.path.join(folder_path_L, 'train_clean.csv'), sep=';', index=True)
# test_clean.to_csv(os.path.join(folder_path_L, 'test_clean.csv'), sep=';', index=True)

