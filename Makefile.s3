# path: Makefile.s3

SHELL := /usr/bin/env bash
.RECIPEPREFIX := >

S3_VENV := .s3venv
S3_PY   := $(if $(wildcard $(S3_VENV)/bin/python),$(S3_VENV)/bin/python,python3)
S3_PIP  := $(if $(wildcard $(S3_VENV)/bin/pip),$(S3_VENV)/bin/pip,pip3)

# Par dÃ©faut: fichier/clÃ© Ã  la racine
S3_FILE  ?= merged_sales_data.csv
S3_KEY   ?= merged_sales_data.csv
S3_DIR   ?= data/                 # dossier local Ã  synchroniser (upload)
S3_PREFIX?= data/                 # prÃ©fixe S3 (dans le bucket)
S3_OUT   ?= downloads/            # dossier local pour les tÃ©lÃ©chargements
S3_EXP   ?= 3600                  # secondes pour presign URL

.PHONY: s3-help s3-venv s3-install s3-env s3-sanity s3-upload s3-upload-mp s3-list s3-download s3-sync-up s3-sync-down s3-presign s3-cat s3-rm s3-clean

s3-help: ## Aide section S3 (DagsHub)
> echo "S3 targets:"
> echo "  s3-install     : crÃ©e venv .s3venv + installe boto3/botocore"
> echo "  s3-sanity      : vÃ©rifie l'accÃ¨s (list 1 objet)"
> echo "  s3-upload      : upload single-part (fiable)"
> echo "  s3-upload-mp   : upload multipart doux"
> echo "  s3-list        : liste un prÃ©fixe"
> echo "  s3-download    : tÃ©lÃ©charge une clÃ© vers S3_OUT"
> echo "  s3-sync-up     : envoie un dossier local vers un prÃ©fixe S3"
> echo "  s3-sync-down   : rÃ©cupÃ¨re un prÃ©fixe S3 vers un dossier local"
> echo "  s3-presign     : URL temporaire de tÃ©lÃ©chargement"
> echo "  s3-cat         : affiche le dÃ©but d'un objet"
> echo "  s3-rm          : supprime une clÃ© ou un prÃ©fixe"
> echo "Vars utiles: S3_FILE S3_KEY S3_DIR S3_PREFIX S3_OUT S3_EXP"

s3-venv:
> python3 -m venv $(S3_VENV)
> $(S3_PIP) -q install --upgrade pip


s3-install: s3-venv
> $(S3_PIP) -q install boto3 botocore pandas click mlflow pyarrow fastparquet

# Vars parquet
PARQ_LOCAL ?= data/merged_sales_data.parquet
CSV_LOCAL  ?= merged_sales_data.csv
PARQ_KEY   ?= $(PARQ_LOCAL)      # mÃªme chemin cÃ´tÃ© S3
CSV_SEP    ?= ;

# Convert CSV(;) -> Parquet (streaming)
.PHONY: s3-csv2parquet-stream
s3-csv2parquet-stream: s3-install
> $(S3_PY) tools/csv_to_parquet.py --src "$(CSV_LOCAL)" --dst "$(PARQ_LOCAL)" --sep "$(CSV_SEP)"

# Upload le Parquet sous data/
.PHONY: s3-upload-parquet
s3-upload-parquet: s3-env
> set -a; source $$HOME/.dagshub.env; set +a; \
> $(S3_PY) tools/dagshub_s3.py upload --file "$(PARQ_LOCAL)" --key "$(PARQ_KEY)" --force-single --verbose

# Pipeline parquet: convert -> upload -> postcheck -> list data/
.PHONY: s3-pipeline-parquet
s3-pipeline-parquet: s3-csv2parquet-stream s3-upload-parquet s3-list
> @echo "âœ… Parquet uploadÃ©. Dans DagsHub â†’ Datasets â†’ Repository Storage: mets prefix= data/ â†’ Build/Index."


s3-env:
> test -f $$HOME/.dagshub.env || (echo "Missing $$HOME/.dagshub.env"; exit 1)
> set -a; source $$HOME/.dagshub.env; set +a; \
> echo "Endpoint: $$AWS_S3_ENDPOINT"; \
> echo "Bucket  : $$DAGSHUB_BUCKET"; \
> echo "Region  : $$AWS_DEFAULT_REGION"

s3-sanity: s3-env
> set -a; source $$HOME/.dagshub.env; set +a; \
> $(S3_PY) tools/dagshub_s3.py sanity

s3-upload: s3-env
> set -a; source $$HOME/.dagshub.env; set +a; \
> $(S3_PY) tools/dagshub_s3.py upload --file "$(S3_FILE)" --key "$(S3_KEY)" --force-single --verbose

s3-upload-mp: s3-env
> set -a; source $$HOME/.dagshub.env; set +a; \
> $(S3_PY) tools/dagshub_s3.py upload --file "$(S3_FILE)" --key "$(S3_KEY)" \
>   --chunk-size-mb 8 --multipart-threshold-mb 16 --max-concurrency 2 --verbose

s3-list: s3-env
> set -a; source $$HOME/.dagshub.env; set +a; \
> $(S3_PY) tools/dagshub_s3.py list --prefix "$(S3_PREFIX)" --max-keys 50

s3-download: s3-env
> mkdir -p "$(S3_OUT)"
> set -a; source $$HOME/.dagshub.env; set +a; \
> $(S3_PY) tools/dagshub_s3.py download --key "$(S3_KEY)" --out "$(S3_OUT)"

s3-sync-up: s3-env
> set -a; source $$HOME/.dagshub.env; set +a; \
> $(S3_PY) tools/dagshub_s3.py sync-up --dir "$(S3_DIR)" --prefix "$(S3_PREFIX)" --force-single

s3-sync-down: s3-env
> mkdir -p "$(S3_OUT)"
> set -a; source $$HOME/.dagshub.env; set +a; \
> $(S3_PY) tools/dagshub_s3.py sync-down --prefix "$(S3_PREFIX)" --out "$(S3_OUT)"

s3-presign: s3-env
> set -a; source $$HOME/.dagshub.env; set +a; \
> $(S3_PY) tools/dagshub_s3.py presign --key "$(S3_KEY)" --expires "$(S3_EXP)"

s3-cat: s3-env
> set -a; source $$HOME/.dagshub.env; set +a; \
> $(S3_PY) tools/dagshub_s3.py cat --key "$(S3_KEY)" --lines 20

s3-rm: s3-env
> set -a; source $$HOME/.dagshub.env; set +a; \
> $(S3_PY) tools/dagshub_s3.py rm --key "$(S3_KEY)" --prefix "$(S3_PREFIX)"

s3-clean:
> rm -rf $(S3_VENV) __pycache__ .pytest_cache


# Deps supplÃ©mentaires pour import_data.py
# (pandas/click/mlflow/pyarrow pour CSV/Parquet; boto3/botocore dÃ©jÃ  installÃ©s)
s3-install-imp: s3-venv
> $(S3_PIP) -q install boto3 botocore pandas click mlflow pyarrow fastparquet

# ParamÃ¨tres par dÃ©faut de l'import (override Ã  l'appel si besoin)

# Lance l'import incrÃ©mental directement depuis DagsHub (source S3)
# Utilise l'env ~/.dagshub.env dÃ©jÃ  chargÃ© par s3-env
.PHONY: s3-import-imp
s3-import: s3-env
> set -a; source $$HOME/.dagshub.env; set +a; \
> $(S3_PY) /home/vboxuser/Compagnon_new/Compagnon_immo_25/mlops/1_import_donnees/import_data.py \
>   --source-mode s3 \
>   --output-folder "$(IMP_OUT_DIR)" \
>   --cumulative-path "$(IMP_CUMUL_PATH)" \
>   --checkpoint-path "$(IMP_CHECKPOINT)" \
>   --sep "$(IMP_SEP)" \
>   --s3-key "$(IMP_S3_KEY)" \
>   $(if $(IMP_DATE_COL),--date-column "$(IMP_DATE_COL)",) \
>   $(if $(IMP_KEY_COLS),--key-columns "$(IMP_KEY_COLS)",)

# ChaÃ®ne complÃ¨te: upload => import => list (vÃ©rif)
.PHONY: s3-pipeline
s3-pipeline: s3-upload s3-import s3-list
> @echo "âœ… Pipeline S3 terminÃ©."
# ============================================================================ 


# ===== Parquet defaults =====
PARQ_LOCAL ?= data/merged_sales_data.parquet
CSV_LOCAL  ?= merged_sales_data.csv
PARQ_KEY   ?= $(PARQ_LOCAL)
CSV_SEP    ?= ;

# s3-install déjà présent: on s'assure d'avoir les deps nécessaires
s3-install: s3-venv
> $(S3_PIP) -q install boto3 botocore pandas click mlflow pyarrow fastparquet

# Convert CSV(;) -> Parquet (streaming)
.PHONY: s3-csv2parquet-stream
s3-csv2parquet-stream: s3-install
> $(S3_PY) tools/csv_to_parquet.py --src "$(CSV_LOCAL)" --dst "$(PARQ_LOCAL)" --sep "$(CSV_SEP)"

# Upload le Parquet sous data/
.PHONY: s3-upload-parquet
s3-upload-parquet: s3-env
> set -a; source $$HOME/.dagshub.env; set +a; \
> $(S3_PY) tools/dagshub_s3.py upload --file "$(PARQ_LOCAL)" --key "$(PARQ_KEY)" --force-single --verbose

# Vérifie le Parquet directement sur S3 (schéma + 5 lignes)
.PHONY: s3-verify-parquet
s3-verify-parquet: s3-env
> set -a; source $$HOME/.dagshub.env; set +a; \
> VERIFY_KEY="$(PARQ_KEY)" $(S3_PY) tools/verify_parquet_s3.py

# Branche s3-import pour lire le Parquet par défaut
IMP_OUT_DIR    ?= data/incremental
IMP_CUMUL_PATH ?= data/df_sample.csv
IMP_CHECKPOINT ?= data/checkpoint.parquet
IMP_DATE_COL   ?=
IMP_KEY_COLS   ?=
IMP_SEP        ?= ;
# IMPORTANT: Par défaut on lit le Parquet uploadé
IMP_S3_KEY     ?= $(PARQ_KEY)

.PHONY: s3-import
s3-import: s3-env
> set -a; source $$HOME/.dagshub.env; set +a; \
> $(S3_PY) /home/vboxuser/Compagnon_new/Compagnon_immo_25/mlops/1_import_donnees/import_data.py \
>   --source-mode s3 \
>   --output-folder "$(IMP_OUT_DIR)" \
>   --cumulative-path "$(IMP_CUMUL_PATH)" \
>   --checkpoint-path "$(IMP_CHECKPOINT)" \
>   --sep "$(IMP_SEP)" \
>   --s3-key "$(IMP_S3_KEY)" \
>   $(if $(IMP_DATE_COL),--date-column "$(IMP_DATE_COL)",) \
>   $(if $(IMP_KEY_COLS),--key-columns "$(IMP_KEY_COLS)",)

# Pipeline parquet complète: convert -> upload parquet -> verify -> import -> list
.PHONY: s3-pipeline-parquet
s3-pipeline-parquet: s3-csv2parquet-stream s3-upload-parquet s3-verify-parquet s3-import s3-list
> @echo "#  Pipeline Parquet terminée (upload+verify+import). Dans DagsHub, datasource prefix=data/ puis Build/Index."

# ===== Parquet defaults =====
PARQ_LOCAL ?= data/merged_sales_data.parquet
CSV_LOCAL  ?= merged_sales_data.csv
PARQ_KEY   ?= $(PARQ_LOCAL)
CSV_SEP    ?= ;

# s3-install déjà présent: on s'assure d'avoir les deps nécessaires
s3-install: s3-venv
> $(S3_PIP) -q install boto3 botocore pandas click mlflow pyarrow fastparquet

# Convert CSV(;) -> Parquet (streaming)
.PHONY: s3-csv2parquet-stream
s3-csv2parquet-stream: s3-install
> $(S3_PY) tools/csv_to_parquet.py --src "$(CSV_LOCAL)" --dst "$(PARQ_LOCAL)" --sep "$(CSV_SEP)"

# Upload le Parquet sous data/
.PHONY: s3-upload-parquet
s3-upload-parquet: s3-env
> set -a; source $$HOME/.dagshub.env; set +a; \
> $(S3_PY) tools/dagshub_s3.py upload --file "$(PARQ_LOCAL)" --key "$(PARQ_KEY)" --force-single --verbose

# Vérifie le Parquet directement sur S3 (schéma + 5 lignes)
.PHONY: s3-verify-parquet
s3-verify-parquet: s3-env
> set -a; source $$HOME/.dagshub.env; set +a; \
> VERIFY_KEY="$(PARQ_KEY)" $(S3_PY) tools/verify_parquet_s3.py

# Branche s3-import pour lire le Parquet par défaut
IMP_OUT_DIR    ?= data/incremental
IMP_CUMUL_PATH ?= data/df_sample.csv
IMP_CHECKPOINT ?= data/checkpoint.parquet
IMP_DATE_COL   ?=
IMP_KEY_COLS   ?=
IMP_SEP        ?= ;
# IMPORTANT: Par défaut on lit le Parquet uploadé
IMP_S3_KEY     ?= $(PARQ_KEY)

.PHONY: s3-import
s3-import: s3-env
> set -a; source $$HOME/.dagshub.env; set +a; \
> $(S3_PY) /home/vboxuser/Compagnon_new/Compagnon_immo_25/mlops/1_import_donnees/import_data.py \
>   --source-mode s3 \
>   --output-folder "$(IMP_OUT_DIR)" \
>   --cumulative-path "$(IMP_CUMUL_PATH)" \
>   --checkpoint-path "$(IMP_CHECKPOINT)" \
>   --sep "$(IMP_SEP)" \
>   --s3-key "$(IMP_S3_KEY)" \
>   $(if $(IMP_DATE_COL),--date-column "$(IMP_DATE_COL)",) \
>   $(if $(IMP_KEY_COLS),--key-columns "$(IMP_KEY_COLS)",)

# Pipeline parquet complète: convert -> upload parquet -> verify -> import -> list
.PHONY: s3-pipeline-parquet
s3-pipeline-parquet: s3-csv2parquet-stream s3-upload-parquet s3-verify-parquet s3-import s3-list
> @echo "#  Pipeline Parquet terminée (upload+verify+import). Dans DagsHub, datasource prefix=data/ puis Build/Index."
